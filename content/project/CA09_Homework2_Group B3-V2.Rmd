---
title: "Session 4: Homework 2"
author: "Study group 3: Ismail Riahi, John Purcell, Parthivi Bansal, Ivo Margetich, Doris Liu, Xinyue Zhang, Jacopo Lorusso Caputi"
date: "`r Sys.Date()`"
output:
  html_document:
    theme: flatly
    highlight: zenburn
    number_sections: yes
    toc: yes
    toc_float: yes
    code_folding: show
---


```{r, setup, include=FALSE}
knitr::opts_chunk$set(
  message = FALSE, 
  warning = FALSE, 
  tidy=FALSE,     # display code as typed
  size="small")   # slightly smaller font for code
options(digits = 3)

# default figure size
knitr::opts_chunk$set(
  fig.width=6.75, 
  fig.height=6.75,
  fig.align = "center"
)
```


```{r load-libraries, include=FALSE}

library(ggtext)
library(tidyverse)  # Load ggplot2, dplyr, and all the other tidyverse packages
library(mosaic)
library(ggthemes)
library(lubridate)
library(here)
library(skimr)
library(janitor)
library(httr)
library(readxl)
library(vroom)
library(infer)
library(rvest)
library(kableExtra)
```



# Task 1: Climate change and temperature anomalies 


First we use read_csv to get the data. When using this function, we added two options: `skip` and `na`.

1. The `skip=1` option is there as the real data table only starts in Row 2, so we need to skip one row. 
2. `na = "***"` option informs R how missing observations in the spreadsheet are coded. When looking at the spreadsheet, you can see that missing data is coded as "***". It is best to specify this here, as otherwise some of the data is not recognized as numeric data.

```{r weather_data, cache=TRUE}

weather <- 
  read_csv("https://data.giss.nasa.gov/gistemp/tabledata_v4/NH.Ts+dSST.csv", 
           skip = 1, 
           na = "***")

```

```{r glimpse and skim weather dataset}
glimpse(weather)
skimr::skim(weather)
```


After selecting the year and the twelve month variables from the `weather` dataset, we convert the dataframe from wide to 'long' format using pivot_longer()` function. We name the new dataframe as `tidyweather`, name the variable containing the name of the month as `month`, and the temperature deviation values as `delta`.

The dataframe now has three variables:
1. year, 
2. month, and 
3. delta, or temperature deviation.

```{r tidyweather}
# 1. Select
weather <- weather %>% 
  select(Year, Jan, Feb, Mar, Apr, May, Jun, Jul, Aug, Sep, Oct, Nov, Dec )

# 2. Long Format
tidyweather <- weather %>% 
  pivot_longer(cols=2:13, 
               names_to="Month", 
               values_to = "Delta")

tidyweather
```


## Plotting weather anomalies

```{r scatter_plot, fig.width=12, fig.height=9}

tidyweather <- tidyweather %>%
  mutate(date = ymd(paste(as.character(Year), Month, "1")),
         month = month(date, label=TRUE),
         year = year(date))

ggplot(tidyweather, aes(x=date, y = Delta))+
  geom_point()+
  geom_smooth(color="red") +
  theme_bw() +
  labs (
    title = "Weather Anomalies"
  )

```

Then we plot to see whether the effect of increasing temperature more pronounced in some months.
```{r facet_wrap, fig.width=12, fig.height=9}

ggplot(tidyweather, aes(x=date, y = Delta))+
  geom_point()+
  geom_smooth(color="red") +
  facet_wrap(~month)+
  theme_bw() +
  labs (
    title = "Weather Anomalies"
  )

```
It seems that increasing temperatures are found in all months with the highest values in February and March

The code below creates a new data frame grouping data in five time periods: 1881-1920, 1921-1950, 1951-1980, 1981-2010 and 2011-present. 
```{r intervals}

comparison <- tidyweather %>% 
  filter(Year>= 1881) %>%     #remove years prior to 1881
  #create new variable 'interval', and assign values based on criteria below:
  mutate(interval = case_when(
    Year %in% c(1881:1920) ~ "1881-1920",
    Year %in% c(1921:1950) ~ "1921-1950",
    Year %in% c(1951:1980) ~ "1951-1980",
    Year %in% c(1981:2010) ~ "1981-2010",
    TRUE ~ "2011-present"
  ))

```

Now we create a density plot to study the distribution of monthly deviations grouped by the different time periods we are interested in. 
```{r density_plot, fig.width=12, fig.height=9}

ggplot(comparison, aes(x=Delta, fill=interval))+
  geom_density(alpha=0.2) +   #density plot with tranparency set to 20%
  theme_bw() +                #theme
  labs (
    title = "Density Plot for Monthly Temperature Anomalies",
    y     = "Density",
    x = "Delta" #changing y-axis label to sentence case
  )

```

So far, we have been working with monthly anomalies. However, we might be interested in average annual anomalies. We can do this by using `group_by()` and `summarise()`, followed by a scatter plot to display the result. 
```{r averaging annual anomolies and plot, eval=FALSE,fig.width=12, fig.height=9}

average_annual_anomaly <- tidyweather %>% 
  group_by(Year) %>%   
  summarise(annual_mean_delta = mean(Delta, na.rm=TRUE)) 

ggplot(average_annual_anomaly, aes(x=Year, y= annual_mean_delta))+
  geom_point()+
  geom_smooth() +
  theme_bw() +
  labs (
    title = "Average Yearly Anomaly",
    y     = "Average Annual Delta"
  )                         


```


## Confidence Interval for `Delta`

[NASA points out on their website](https://earthobservatory.nasa.gov/world-of-change/decadaltemp.php) that 
> A one-degree global change is significant because it takes a vast amount of heat to warm all the oceans, atmosphere, and land by that much. In the past, a one- to two-degree drop was all it took to plunge the Earth into the Little Ice Age.

We constructed a confidence interval for the average annual delta since 2011.
```{r, calculate_CI_using_formula, eval=FALSE}
formula_ci <- comparison %>% 
  filter(interval=="2011-present") %>% 
  summarize(mean_delta = mean(Delta, na.rm=TRUE), 
            sd_delta = sd(Delta, na.rm=TRUE),
            count=n(),
            se_delta = sd_delta/sqrt(count),
            t_critical = qt(0.975, count-1),
            upper_bound = mean_delta+t_critical*se_delta,
            lower_bound = mean_delta-t_critical*se_delta
            )
  
formula_ci
```


```{r, calculate_CI_using_bootstrap}
set.seed(1234)
bootstrap_delta <- comparison %>% 
  filter(interval=="2011-present") %>% 
  specify(response = Delta) %>% 
  generate(reps=10000, type="bootstrap") %>% 
  calculate(stat=c("mean"))
  
bootstrap_ci <- bootstrap_delta %>% 
  get_ci(level=0.95, type="percentile")

bootstrap_ci
```
> The data points constitute the confidence interval, that we are 95% sure that it includes the true mean annual delta.


# Task 2: Global warming and political views (GSS)

[A 2010 Pew Research poll](https://www.pewresearch.org/2010/10/27/wide-partisan-divide-over-global-warming/) asked 1,306 Americans, "From what you've read and heard, is there solid evidence that the average temperature on earth has been getting warmer over the past few decades, or not?"

Here we analyze whether there are any differences between the proportion of people who believe the earth is getting warmer and their political ideology. From the **survey sample data**, we will use the proportions to estimate values of *population parameters*. The file has 2253 observations on the following 2 variables:

- `party_or_ideology`: a factor (categorical) variable with levels Conservative Republican, Liberal Democrat, Mod/Cons Democrat, Mod/Lib Republican
- `response` : whether the respondent believes the earth is warming or not, or Don't know/ refuse to answer

```{r, read_global_warming_pew_data}
global_warming_pew <- read_csv(here::here("data_project", "global_warming_pew.csv"))
glimpse(global_warming_pew)
```

```{r, count different responses of different parties}
global_warming_pew <- global_warming_pew %>% 
  filter(response!="Don't know / refuse to answer") %>%
  count(party_or_ideology, response)

global_warming_pew

```


We then construct 95% confidence intervals to estimate population parameters, for the % who believe that **Earth is warming**, according to their party or ideology. 
Methods include creating the CIs using the formulas by hand, or using `prop.test()`.
```{r, }
global_warming_pew %>% 
  filter(response==c("Earth is warming", "Not warming")) %>%
  # count(party_or_ideology, response) %>% 
  pivot_wider(names_from = response, values_from=n) %>% 
  clean_names() %>%
  mutate(
    total=earth_is_warming + not_warming,
    prop = earth_is_warming/total,
    se=sqrt(prop*(1-prop)/total),
    lower_bound = prop - 1.96*se,
    upper_bound = prop + 1.96*se
  )
 
```

We constructed 95% confidence intervals to estimate population parameters, for the % who believe that **Earth is warming**, according to their party or ideology. 

It appers that beliefs on climate change are different according to political ideology - the more liberal a person the more probable they are to believe in climate change as opposed to conservatives tending to discard the statement that earth is warming.  

For further information read [The challenging politics of climate change](https://www.brookings.edu/research/the-challenging-politics-of-climate-change/)



# Task 3: Biden's Approval Margins

We import Biden's approval dataset from fivethirtyeight.com [all polls that track the president's approval ](https://projects.fivethirtyeight.com/biden-approval-ratings), and use `lubridate` to fix dates.

```{r, import biden approval dataset and fix dates, cache=TRUE}
# Import approval polls data directly off fivethirtyeight website
approval_polllist <- read_csv('https://projects.fivethirtyeight.com/biden-approval-data/approval_polllist.csv') 

glimpse(approval_polllist)
skim(approval_polllist)

# Use `lubridate` to fix dates, as they are given as characters.
approval_polllist <- approval_polllist %>%
  mutate(modeldate = mdy(modeldate)) %>%
  mutate(startdate = mdy(startdate)) %>%
  mutate(enddate = mdy(enddate)) %>%
  mutate(createddate = mdy(createddate))
str(approval_polllist)
```

## Average net approval rate

Then we calculate the average net approval rate (approve- disapprove) for each week since he got into office (Jan 20th 2021), and get the mean, standard deviation, and 95% confidence interval of the weekly net approval. 
```{r calculate the average net approval rate}
approval_polllist_avg_approval <- approval_polllist %>%
  mutate(net_approval = approve-disapprove) %>%
  mutate(end_week = week(enddate)) %>%
  group_by(end_week)%>%
  summarise(average_net_approval = mean(net_approval),
            sd_net_approval = sd(net_approval),
            count=n(),
            se_net_approval = sd_net_approval/sqrt(count),
            upper_bound = average_net_approval+se_net_approval*qt(0.975, count-1),
            lower_bound = average_net_approval-se_net_approval*qt(0.975, count-1))

approval_polllist_avg_approval
  
```

Biden's weekly average net approval rate plot is then generated, along with the smooth line and 95% confidence interval.
```{r plot the average net approval rate, fig.width=12, fig.height=9}
ggplot(approval_polllist_avg_approval, aes(x=end_week,y=average_net_approval)) +
  geom_point(color="red")+
  geom_line(color="red")+
  geom_smooth(se=F,span=1)+
  geom_ribbon(aes(ymin=lower_bound,ymax=upper_bound),alpha=0.1,color="grey")+
  scale_y_continuous(limits=c(-10,25))+
  geom_hline(yintercept=0,color="orange",size=1.5)+
  theme_minimal()  +
  theme(plot.title =element_text(size=16, face='bold',hjust = 0.5,margin = margin(10,0,10,0)),
        plot.subtitle =element_text(size=16, face='bold',hjust = 0.5), #put titles in the middle
        axis.text.x = element_text(size=10),
        axis.text.y = element_text(size=12),
        axis.ticks.x = element_line(),
        axis.ticks.y=element_line(),
        axis.title.x = element_text(size=16,face='bold'),
        axis.title.y = element_text(size=16,face='bold'),
        ) +
  labs(title = "Biden Net Approval Rate",
         subtitle = "Weekly Average",
         x = "Week",
         y = "",
         caption = "Source: https://projects.fivethirtyeight.com/biden-approval-ratings") +
  ylab("Average net approval rate")
  
```


## Confidence Intervals for Week 3 and Week 25
                                                              
The confidence intervals for week 3 and week 25 are respectively the largest and the smallest. In week 3 the confidence interval ranges between 13 and 24, whereas in week 25 the lower bound is 10 and the upper bound 13. The difference is due to the sample size of the data. The much higher number of values for week 25 decreases exponentially the standard error, that is in fact 0.6 compared to the 2.5 for week 3.


# Challenge 1: Excess rentals in TfL bike sharing

Recall the TfL data on how many bikes were hired every single day. We can get the latest data by running the following codes:
```{r, get_tfl_data, cache=TRUE}
url <- "https://data.london.gov.uk/download/number-bicycle-hires/ac29363e-e0cb-47cc-a97a-e216d900a6b0/tfl-daily-cycle-hires.xlsx"

# Download TFL data to temporary file
httr::GET(url, write_disk(bike.temp <- tempfile(fileext = ".xlsx")))

# Use read_excel to read it as dataframe
bike0 <- read_excel(bike.temp,
                   sheet = "Data",
                   range = cell_cols("A:B"))

# change dates to get year, month, and week
bike <- bike0 %>% 
  clean_names() %>% 
  rename (bikes_hired = number_of_bicycle_hires) %>% 
  mutate (year = year(day),
          month = lubridate::month(day, label = TRUE),
          week = isoweek(day))
```


We can easily create a facet grid that plots bikes hired by month and year.


The distributions of bikes hired per month during May and Jun in 2020 is flatter compared with that of the previous years. The standard deviations of May and June, 2020, are also higher than the previous years, reflecting the fact that there more variations among days when very few bikes were rented and days when lots of bikes were rented. This wide dispersion is the evidence of Covid-19's significant affect and restrictionson on people's daily travel.


We then start to reproduce the following two graphs: 

1. Monthly changes in TFL bike rentals between 2016 and 2019

2. TFL bike rentals' weekly percentage changes from the expected rentals between 2016 and 2019.
The two grey shaded rectangles correspond to Q2 (weeks 14-26) and Q4 (weeks 40-52).


For both of these graphs, we calculate the expected number of rentals per week or month between 2016-2019, and then see how each week/month of 2020-2021 compares to the expected rentals, using the calculation `excess_rentals = actual_rentals - expected_rentals`. 


The mean of the number of bicycle hired is used to calculate the expected rentals, as mean takes the whole dataset into consideration and reprsents the average of the entire data. 

Additionally, we uses these links as references when creating plots:

- https://ggplot2.tidyverse.org/reference/geom_ribbon.html
- https://ggplot2.tidyverse.org/reference/geom_tile.html 
- https://ggplot2.tidyverse.org/reference/geom_rug.html


We first calculate the mean of monthly number of bicycles hired between 2016 and 2019, and then get the monthly changes in TFL bike rentals using excess_rental method. We set "up" for the positive excess rental (when monthly actual_rentals is greater than expected_rentals), and "down" for the negative excess rental (when monthly actual_rentals is less than expected_rentals).

```{r monthly bike rental change, out.width="100%"}
bike_month_16_19 <- bike %>%
  filter(year>=2016&year<=2019)%>%
  group_by(month) %>%
  summarise(expected_rental=mean(bikes_hired))


bike_month <- bike %>%
  filter(year>=2016) %>%
  group_by(year,month) %>%
  summarise(bike_hired_month=mean(bikes_hired),.groups = 'drop')

bike_month_comp <- merge(bike_month,bike_month_16_19,by="month") %>%
  mutate(excess_rentals = bike_hired_month - expected_rental,
         up = ifelse(bike_hired_month>expected_rental, excess_rentals, 0), #up gives the diffrence between actual and expected rentals when actual>expected
         down = ifelse(bike_hired_month<expected_rental, excess_rentals, 0)) #down gives the diffrence between actual and expected rentals when actual<expected

```

Then we use geom_line and geom_ribbon to generate the lines for expected rental. The green area represents the rental changes where the actual monthly number of bicycles hired is greater than the expected rental, and the red area appears when the actual monthly number of bicycles hired is less than the expected rental. The plots of monthly changes in Tfl bike rentals between 2016 and 2019 are illustrated below:

```{r plot monthly bike rental change, out.width="100%",fig.width=15, fig.height=9}
ggplot(bike_month_comp,aes(month))+
  geom_line(aes(x=month,y=expected_rental,colour="Expected",group=year),size=1)+
  geom_line(aes(x=month,y=bike_hired_month,colour="Actual",group=year))+
  geom_ribbon(aes(ymin=expected_rental,
                  ymax=expected_rental+up,group=year),alpha=0.4,fill="#7DCD85")+ #plot the areas in green using up when actual>expected
  geom_ribbon(aes(ymin=expected_rental+down,
                  ymax=expected_rental,group=year),alpha=0.4,fill="#CB454A")+ #plot the areas in red when using down actual>expected
  scale_colour_manual("",breaks=c("Expected","Actual"),values=c("blue","black"))+ 
  facet_wrap(~year)+
  theme_minimal()  +
  theme(legend.position = "none",
        plot.title =element_text(size=16, face='bold',hjust = 0,margin = margin(10,0,10,0)),
        plot.subtitle =element_text(size=16, hjust = 0), #put titles in the middle
        axis.text.x = element_text(size=10),
        axis.text.y = element_text(size=12),
        axis.ticks.x = element_line(),
        axis.ticks.y=element_line(),
        axis.title.x = element_text(size=16,face='bold'),
        axis.title.y = element_text(size=16,face='bold'),
        ) +
  labs(title = "Monthly changes in TfL bike rentals", 
       subtitle = "Expected rentals shown in blue and calculated between 2016-2019, Actual rentals shown in black", 
       caption= "Source: TfL, London Data Store",
       x="Month", y="Bike Rentals") 

```


We then calculate the mean of weekly number of bicycles hired, and the weekly percentage change between actual and expected bike rentals between 2016 and 2019. We set "up" for the positive percentage change (when weekly actual_rentals is greater than expected_rentals), and "down" for the negative percentage change (when weekly actual_rentals is less than expected_rentals).

```{r weekly bike rental change, out.width="100%"}
glimpse(bike)
bike_week_16_19 <- bike %>%
  filter(year>=2016&year<=2019)%>%
  group_by(week) %>%
  summarise(expected_rental=mean(bikes_hired))


bike_week <- bike %>%
  filter(year>=2016) %>%
  filter(!(year==2021&week==53))%>%
  group_by(year,week) %>%
  summarise(bike_hired_week=mean(bikes_hired),.groups = 'drop')

bike_week_comp <- merge(bike_week,bike_week_16_19,by="week") %>%
  mutate(percentage_rentals_change = (bike_hired_week - expected_rental)/expected_rental,
         up = ifelse(bike_hired_week>expected_rental, percentage_rentals_change, 0), #up gives the percentage diffrence between actual and expected rentals when actual>expected
         down = ifelse(bike_hired_week<expected_rental, percentage_rentals_change, 0)) #down gives the percentage diffrence between actual and expected rentals when actual<expected

```

The two grey shaded rectangles correspond to Q2 (weeks 14-26) and Q4 (weeks 40-52) are also added.
```{r plot weekly bike rental change, out.width="100%",fig.width=15, fig.height=12}
ggplot(bike_week_comp,aes(week))+
  geom_rect(aes(xmin=14,xmax=26,ymin=-0.8,ymax=1.1,group=year),colour="grey",alpha=0.002)+ #plot rectangles for Q2 
  geom_rect(aes(xmin=40,xmax=52,ymin=-0.8,ymax=1.1,group=year),colour="grey",alpha=0.002)+ #plot rectangles for Q4
  geom_rug(aes(colour=ifelse(bike_hired_week>=expected_rental,">=0","<0")),sides="b")+ #plot rug for x asis
  scale_colour_manual(values=c("#CB454A","#7DCD85"),name="Actual vs Expected ", guide=FALSE)+
  geom_line(aes(x=week,y=percentage_rentals_change,group=year),colour="black",size=0.5)+
  geom_ribbon(aes(ymin=0,
                  ymax=up,group=year),alpha=0.4,fill="#7DCD85")+ #plot the areas in green using up when actual>expected
  geom_ribbon(aes(ymin=down,
                  ymax=0,group=year),alpha=0.4,fill="#CB454A")+ #plot the areas in red using up when actual>expected
  facet_wrap(~year)+
  theme_minimal()  +
  theme(legend.position = "none",
        plot.title =element_text(size=16, face='bold',hjust = 0,margin = margin(10,0,10,0)),
        plot.subtitle =element_text(size=16, hjust = 0), #put titles in the middle
        axis.text.x = element_text(size=10),
        axis.text.y = element_text(size=12),
        axis.ticks.x = element_line(),
        axis.ticks.y=element_line(),
        axis.title.x = element_text(size=16,face='bold'),
        axis.title.y = element_text(size=16,face='bold'),
        ) +
  labs(title = "Weekly changes in TfL bike rentals", 
       subtitle = "Percentage changes from the expected level of weekly rentals", caption= "Source: TfL, London Data Store",
       x="Week", y="Percentage Change in Bike Rentals") +
  scale_x_continuous(breaks=c(0,13,26,39,53),limits=c(0,53))+
  scale_y_continuous(labels=scales::percent)

```


# Challenge 2: How has the CPI and its components changed over the last few years?

1. We scrape the FRED website and pull all of the CPI components into a vector.
```{r, scape CPI data}
library(rvest)
url <- "https://fredaccount.stlouisfed.org/public/datalist/843"

tables <- url %>% 
  read_html() %>% 
  html_nodes(css="table")

cpi <- map(tables, . %>% 
             html_table(fill=TRUE)%>% 
             janitor::clean_names()) 

cpi_id = cpi[[2]]$series_id

```

2. Once we have a vector of components, you can then pass it to `tidyquant::tq_get(get = "economic.data", from =  "2000-01-01")` to get all data since January 1, 2000
```{r, get all data since January 1, 2000}
cpi_data <- tidyquant::tq_get(cpi_id, get = "economic.data", from =  "2000-01-01")
```


3. Since the data you download is an index with various starting dates, we need to calculate the yearly, or 12-month change. To do this we need to use the `lag` function, and specifically, `year_change = value/lag(value, 12) - 1`; this means that we are comparing the current month's value with that 12 months ago lag(value, 12).
```{r, calculate year change}
cpi_data_clean <- cpi_data %>% pivot_wider(id_cols=1:3, 
               names_from="symbol", 
               values_from = "price")

#Make date the index to make mutate_all work
cpi_data_clean1 <- cpi_data_clean %>% 
     remove_rownames() %>%
     column_to_rownames(var = 'date')

#Define Function
Funk <- function(x, na.rm = FALSE) (x/lag(x, 12) - 1)

#Apply functions to all columns into new dataset
cpi_changes <- cpi_data_clean1 %>%  mutate_all(
  Funk
)

```

Below we format the graph

4. We order components so the higher the yearly change, the earlier does that component appear.
5. We also make sure that the **All Items** CPI (CPIAUCSL) appears first.
6. We then add a `geom_smooth()` for each component to get a sense of the overall trend.
7  Finally, we colour the points according to whether yearly change was positive or negative. 

```{r, format the graph, fig.width=15, fig.height=12}
# Make date back as first column for pivot_longer
cpi_changes <- cbind("Date" = rownames(cpi_changes), cpi_changes)

cpi_changes_graph <- cpi_changes %>% 
  pivot_longer(cols=2:50, 
               names_to="Component", 
               values_to = "YoY")

cpi_changes_graph <- cpi_changes_graph %>% mutate(
  Neg = (YoY<=0)
)

cpi_changes_graph$Date <- as.Date(cpi_changes_graph$Date, "%Y-%m-%d")

# Get our order of components by max descending (excl. All Items)
cpi_order <- cpi_changes_graph %>%
  filter(Date>="2016-01-01") %>%
  filter(Component != "CPIAUCSL") %>% 
  group_by(Component) %>% 
  summarise(max_YoY = max(YoY,na.rm=TRUE)) %>% 
  arrange(-max_YoY)

# Put the all items at the start (append)
order = c(cpi_order$Component)
order = append("CPIAUCSL", order)
order

cpi_changes_graph %>%
  filter(Date>="2016-01-01") %>% 
  group_by(Component) %>% 
  mutate(
    max_cpi = max(YoY)
  )
  
# Graph
cpi_changes_graph %>%
  filter(Date>="2016-01-01") %>% 
  ggplot(aes(Date, YoY)) + 
  geom_point(size=1, aes(colour = Neg)) +
  geom_smooth(se=F, colour="grey") +
  facet_wrap(~factor(Component, levels = order),scales="free")+
  scale_y_continuous(labels = scales::percent)+
  scale_x_date(breaks = as.Date(c("2016-01-01", "2018-01-01","2020-01-01")), labels=c("2016", "2018", "2020"),
               minor_breaks = as.Date(c("2016-01-01", "2018-01-01",
                                        "2020-01-01")))+
  theme_bw()+
  theme(legend.title = element_blank(),legend.position = "none",
        axis.text.x = element_text(size=5, colour = "black"), 
    axis.text.y = element_text(size=5, colour = "black"),
    plot.title = element_text(size=14, face= "bold", colour= "black"),
    plot.subtitle = element_text(size=10, colour= "black"),
    axis.title.x = element_text(size=10, colour = "black"),    
    axis.title.y = element_text(size=10, colour = "black"))+
  
  labs(title="Yearly change of US CPI (All Items) and its components", 
       subtitle = "Year on year change being positive or negative
Jan 2016 to Aug 2021", 
       x="Year", 
       y="YoY % Change")

                     
                     
```

As you can see, we get a similar graph to the target figure below, with identical layout of the facetted graphs. Note that the individual graph headings below were not available in the dataset scraped from FRED, so we used the component IDs instead. 
```{r cpi_all_components_since_2016, echo=FALSE, out.width="100%"}
knitr::include_graphics(here::here("images_project", "cpi_components_since_2016.png"))
```

We now graph the major categories (Housing, Transportation, Food and beverages, Medical care, Education and communication, Recreation, and Apparel), sorted according to their relative importance.
```{r, plot yearly change in CPI using major categories, fig.width=12, fig.height=12}
# Get the codes/series ids for the relevant names
# "Consumer Price Index for All Urban Consumers: Housing in U.S. City Average"             # 42.385, CPIHOSSL
# "Consumer Price Index for All Urban Consumers: Transportation in U.S. City Average"      # 15.160, CPITRNSL
# "Consumer Price Index for All Urban Consumers: Food and Beverages in U.S. City Average"  # 15.157, CPIFABSL
# "Consumer Price Index for All Urban Consumers: Apparel in U.S. City Average"             # 2.663,  CPIAPPSL
# No education, communication, and recreation info in dataset

important_components = c("CPIAUCSL", "CPIHOSSL", "CPITRNSL", "CPIFABSL", "CPIAPPSL") # Included overall figure also

# Graph
cpi_changes_graph %>%
  filter(Date>="2016-01-01") %>% 
  filter(Component==important_components) %>% 
  ggplot(aes(Date, YoY)) + 
  geom_point(size=2, aes(colour = Neg)) +
  geom_smooth(se=F, colour="grey") +
  facet_wrap(~factor(Component, levels = important_components),scales="free", ncol=1)+
  scale_y_continuous(labels = scales::percent)+
  scale_x_date(breaks = as.Date(c("2016-01-01", "2018-01-01","2020-01-01")), labels=c("2016", "2018", "2020"),
               minor_breaks = as.Date(c("2016-01-01", "2018-01-01",
                                        "2020-01-01")))+
  theme_bw()+
  theme(legend.title = element_blank(),legend.position = "none",
        axis.text.x = element_text(size=5, colour = "black"), 
    axis.text.y = element_text(size=5, colour = "black"),
    plot.title = element_text(size=14, face= "bold", colour= "black"),
    plot.subtitle = element_text(size=10, colour= "black"),
    axis.title.x = element_text(size=10, colour = "black"),    
    axis.title.y = element_text(size=10, colour = "black"))+
  
  labs(title="Yearly change of US CPI and selected components of US CPI", 
       subtitle = "Year on year change being positive or negative Jan 2016 to Aug 2021. Ranked in order of relative importantce", 
       x="Year", 
       y="YoY % Change")

```



# Details

- Who did you collaborate with: Catherine Xinyue Zhang, Doris Liu, Ismail Riahi, Ivo Margetich, Jacopo Lorusso Caputi, John Purcell, Parthivi Bansal (Alphabetical order)
- Approximately how much time did you spend on this problem set: 12 hours
- What, if anything, gave you the most trouble: Details of different functions in plots

> As a true test to yourself, do you understand the code you submitted and are you able to explain it to someone else? 
Yes
